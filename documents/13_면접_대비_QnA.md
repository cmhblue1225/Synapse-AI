# 🎤 13. 면접 대비 Q&A

## 📋 프로젝트 개요 (10문)

### Q1. 이 프로젝트를 한 문장으로 설명한다면?
> "AI가 자동으로 파일을 분석하고, 관련 지식을 연결하며, 자연어 질문에 답변하는 **개인 지식 관리 시스템**입니다."

### Q2. 왜 이 프로젝트를 시작했나요?
> "기존 Notion, Obsidian은 수동으로 링크를 생성해야 하고, AI 기능이 제한적입니다. **자동화 + AI 채팅 + 벡터 검색**을 통합하여 차별화된 솔루션을 만들고 싶었습니다."

### Q3. 프로젝트 기간과 개발 인원은?
> "**2개월** (2024년 12월 ~ 2025년 1월), **1인 풀스택 개발**입니다. 기획, 설계, 개발, 배포까지 모두 수행했습니다."

### Q4. 기술 스택을 선택한 이유는?
> "**React 19**는 최신 기능 활용, **TypeScript**는 타입 안전성, **Supabase**는 서버리스 통합 백엔드, **OpenAI API**는 강력한 AI 기능을 제공합니다. 모두 **최신 기술**이면서 **생산성이 높은** 조합입니다."

### Q5. 가장 자랑하고 싶은 기술적 성과는?
> "1) PDF 텍스트 추출의 **CSP 문제를 Edge Function으로 해결**, 2) AI 퀴즈 생성 성공률을 **60%에서 95%로 개선**, 3) 자동 링크 생성으로 **지식 네트워크 구축 시간 95% 단축**입니다."

### Q6. 프로젝트의 비즈니스 가치는?
> "지식 정리 시간 **80% 단축**, 정보 검색 시간 **70% 단축**으로 개인 생산성을 극대화합니다. B2B SaaS나 교육 플랫폼으로 확장 가능합니다."

### Q7. 프로덕션 배포 상태는?
> "**Netlify**에서 안정적으로 운영 중입니다 (https://synapse-doc.netlify.app). GitHub 연동으로 코드 푸시 시 자동 배포되며, Lighthouse 점수 **92/100**을 달성했습니다."

### Q8. 이 프로젝트에서 배운 가장 큰 교훈은?
> "AI 모델의 불완전한 출력에 대비한 **강건한 에러 처리**와 **2단계 파싱 시스템**의 중요성을 깨달았습니다. 또한 서버리스 아키텍처의 **비용 효율성**과 **확장성**을 체감했습니다."

### Q9. 유사한 서비스와의 차별점은?
> "**Notion AI**: 기본 요약만 제공 / **Obsidian**: 수동 링크 생성 / **Synapse**: **자동화 + RAG 채팅 + 벡터 검색** 통합으로 차별화됩니다."

### Q10. 프로젝트의 향후 확장 계획은?
> "단기: 멀티모달 지원 (이미지 OCR, 오디오 Whisper), 중기: 크롬 확장 프로그램, 장기: B2B SaaS (팀 협업 기능)입니다."

---

## 🤖 AI 기술 (15문)

### Q11. OpenAI API를 어떻게 통합했나요?
> "OpenAI SDK를 사용하여 **GPT-4o-mini** (텍스트 생성)와 **text-embedding-3-small** (벡터 임베딩)을 통합했습니다. 클라이언트에서 직접 호출하지만, 향후 Edge Functions로 이동할 계획입니다."

### Q12. GPT-4 대신 GPT-4o-mini를 선택한 이유는?
> "**비용 효율성** (GPT-4 대비 1/10)과 **빠른 응답 속도**입니다. 현재 작업에는 GPT-4o-mini의 성능이 충분하며, 실제 정확도도 90% 이상입니다."

### Q13. Temperature를 0.3에서 0.7까지 다르게 설정한 이유는?
> "기능의 **목적**에 따라 최적화했습니다. 관계 분석(0.3)은 정확한 유형이 중요하고, 요약(0.5)은 일관성이 필요하며, 채팅(0.7)은 자연스러운 대화가 중요합니다."

### Q14. max_tokens를 어떻게 결정했나요?
> "**비용**과 **품질**의 균형입니다. 태그 생성(100) < 요약(150-200) < 퀴즈 생성(300-500) < RAG 채팅(500)으로 용도에 맞게 설정했습니다."

### Q15. AI 퀴즈 생성 성공률을 60%에서 95%로 올린 방법은?
> "GPT-4o-mini의 불일치한 응답을 처리하기 위해 **2단계 JSON 파싱 시스템**을 구현했습니다. 1단계에서 코드 블록 제거, 2단계에서 정규식 백업 파싱으로 성공률이 **35%p 향상**되었습니다."

### Q16. 프롬프트 엔지니어링에서 가장 중요한 원칙은?
> "**명확한 역할 정의**와 **출력 형식 명시**입니다. 시스템 메시지에서 AI의 역할을 정확히 정의하고, Few-shot Learning으로 예시를 제공하면 일관된 응답을 얻을 수 있습니다."

### Q17. OpenAI API 비용을 어떻게 최적화했나요?
> "1) 텍스트 길이를 **6000자로 제한**, 2) React Query로 **24시간 캐싱**, 3) **GPT-4o-mini** 사용으로 GPT-4 대비 90% 비용 절감. 월 평균 API 비용은 약 **$5**입니다."

### Q18. AI 모델의 할루시네이션을 어떻게 방지했나요?
> "1) **프롬프트에 명시적 지시** ('지식에 없으면 모른다고 답변'), 2) **출처 표시 강제**, 3) **낮은 유사도 경고**로 3단계 방어합니다. 실제 할루시네이션률은 **5% 미만**입니다."

### Q19. RAG 시스템을 간단히 설명한다면?
> "AI가 답변하기 전에 **사용자의 지식 데이터베이스를 검색**하여 관련 정보를 찾고, 그 정보를 바탕으로 답변하는 시스템입니다. 이를 통해 할루시네이션을 방지하고 출처를 명확히 할 수 있습니다."

### Q20. Similarity threshold를 0.3으로 낮춘 이유는?
> "높은 threshold(0.7)는 정확도는 높지만 **'관련 지식 없음' 응답이 60%**였습니다. 0.3으로 낮추면 재현율이 높아져 충분한 컨텍스트를 확보하면서도, **Top-3 제한**으로 노이즈를 방지합니다."

### Q21. RAG와 Fine-tuning의 차이는?
> "**Fine-tuning**은 모델 자체를 재학습시켜 비용과 시간이 많이 들고, 최신 정보 반영이 어렵습니다. **RAG**는 모델은 그대로 두고 검색 데이터만 업데이트하여 **비용 효율적**이고 **실시간 반영**이 가능합니다."

### Q22. text-embedding-3-small을 선택한 이유는?
> "**ada-002 대비 5배 저렴**하면서도 성능은 오히려 더 우수합니다 (MTEB 벤치마크 62.3% vs 61.0%). 1536차원으로 충분한 표현력을 제공합니다."

### Q23. Top-K를 3으로 설정한 이유는?
> "K=1은 답변 품질 낮음, K=5 이상은 개선 미미하지만 비용/속도만 증가합니다. **K=3이 최적 균형**입니다."

### Q24. 임베딩 차원이 1536인 이유는?
> "OpenAI **text-embedding-3-small** 모델의 출력 차원입니다. 3072차원의 large 모델 대비 **성능은 거의 동일하면서 저장 공간은 50% 절약**됩니다."

### Q25. AI 응답에서 마크다운 문자를 제거한 이유는?
> "UI에서 일반 텍스트로 표시하는데 마크다운 기호(`*`, `#`)가 보이면 **사용자 경험이 나쁩니다**. 시스템 프롬프트에서 마크다운 사용 금지를 명시하여 해결했습니다."

---

## 🔍 벡터 검색 (10문)

### Q26. 벡터 검색의 장점은?
> "**의미 기반 검색**으로 키워드가 정확히 없어도 관련 문서를 찾을 수 있습니다. '리액트 함수형 컴포넌트 상태 관리'로 검색하면 'React Hooks' 문서도 찾을 수 있죠. **재현율이 높아** 사용자 경험이 좋습니다."

### Q27. 코사인 유사도를 직접 구현한 이유는?
> "pgvector가 서버 사이드에서 계산하지만, **클라이언트에서 캐싱된 벡터를 재검색**할 때도 필요합니다. 또한 디버깅과 테스트를 위해 직접 구현했습니다."

### Q28. IVFFlat을 HNSW 대신 선택한 이유는?
> "HNSW는 더 빠르지만 **메모리 기반**으로 리소스를 많이 사용합니다. 현재 데이터셋 규모(< 10만 개)에서는 IVFFlat의 **98% 정확도**와 **디스크 기반 저장**이 더 적합하고 비용 효율적입니다."

### Q29. IVFFlat의 lists = 100은 어떻게 결정했나요?
> "데이터를 **100개 클러스터로 분할**하여 검색합니다. 권장 공식은 `lists = rows / 1000`이므로, 데이터셋 크기가 10만 개 수준이면 **lists = 100**이 최적값입니다."

### Q30. 벡터 임베딩 생성 시 텍스트를 어떻게 전처리하나요?
> "1) **HTML 태그 제거**, 2) **여러 공백을 하나로**, 3) **앞뒤 공백 제거**, 4) **6000자 제한** (OpenAI 토큰 한도 고려). 이를 통해 일관된 임베딩 품질을 유지합니다."

### Q31. 멀티모달 임베딩이란?
> "**제목 + 내용 + 파일 텍스트**를 결합하여 하나의 벡터로 만듭니다. 구분자 `\\n\\n---\\n\\n`로 각 부분을 명확히 구분하여 더 포괄적인 검색이 가능합니다."

### Q32. 배치 임베딩 처리는 왜 필요한가요?
> "OpenAI API는 한 번에 최대 2048개까지 처리할 수 있습니다. **100개씩 배치 처리**하고 **100ms 대기**하여 rate limit를 회피하면서 효율적으로 처리합니다."

### Q33. pgvector 인덱스로 검색 속도가 얼마나 향상되었나요?
> "인덱스 없음: **2500ms** (전체 스캔), IVFFlat 인덱스: **25ms**. **100배 속도 향상**입니다."

### Q34. 벡터 검색의 한계는?
> "**정확 키워드 매칭이 필요한 경우**에는 약합니다. 예를 들어 '`React.useState`' 같은 코드 검색은 키워드 검색이 더 좋을 수 있습니다. 하이브리드 검색(키워드 + 벡터)이 이상적입니다."

### Q35. 코사인 유사도 범위와 의미는?
> "`1.0` = 완전히 동일, `0.9` = 매우 유사, `0.7` = 유사, `0.5` = 어느 정도 관련, `0.3` = 약간 관련, `0.0` = 무관함, `-1.0` = 완전히 반대"

---

## 📄 PDF 처리 (5문)

### Q36. PDF.js가 실패한 이유와 해결 방법은?
> "브라우저의 **Content Security Policy**가 PDF.js Worker 스크립트 로딩을 차단했습니다. 이를 **Supabase Edge Functions**로 서버사이드 처리하여 해결했고, Deno 런타임에서 pdf-parse npm 패키지를 사용하여 **100% 성공률**을 달성했습니다."

### Q37. Edge Functions의 장점은?
> "1) **글로벌 CDN 배포**로 낮은 레이턴시, 2) **자동 스케일링**, 3) **서버 관리 불필요**, 4) **npm 패키지 지원**으로 기존 생태계 활용 가능합니다."

### Q38. 파일 크기 제한을 어떻게 처리했나요?
> "Supabase의 **10MB 제한**을 고려하여 클라이언트에서 파일 크기를 검증하고, 텍스트는 **150,000자로 제한**하여 OpenAI 토큰 한도를 준수합니다."

### Q39. Edge Functions을 언제 사용하나요?
> "클라이언트에서 처리할 수 없거나 **보안상 민감한 작업**을 Edge Functions로 분리합니다. PDF 텍스트 추출, 향후 OpenAI API 키 보호 등에 사용합니다."

### Q40. 다양한 파일 형식을 어떻게 지원하나요?
> "**PDF**: Edge Function (pdf-parse), **TXT/MD**: 클라이언트 처리, **JSON**: JSON.parse(), **CSV**: 수동 파싱. 5가지 형식을 지원하며, 파일 타입별로 최적의 처리 방법을 선택합니다."

---

## 🏗️ 아키텍처 & 배포 (10문)

### Q41. 서버리스 아키텍처를 선택한 이유는?
> "**비용 효율성**과 **확장성**입니다. Supabase는 PostgreSQL, 인증, 스토리지를 통합 제공하고, Netlify는 글로벌 CDN과 자동 CI/CD를 제공하여 **인프라 관리 부담**을 줄였습니다."

### Q42. 프론트엔드와 백엔드 간 통신은 어떻게 이루어지나요?
> "Supabase 클라이언트 SDK를 통해 **REST API** 호출과 **WebSocket** 실시간 구독을 사용합니다. React Query로 서버 상태를 캐싱하여 불필요한 네트워크 요청을 줄였습니다."

### Q43. RLS로 어떻게 데이터를 보호하나요?
> "**Row Level Security**는 SQL 레벨에서 데이터 접근을 제어합니다. `auth.uid() = user_id` 조건으로 사용자는 자신의 데이터만 조회/수정할 수 있고, **애플리케이션 코드가 아닌 데이터베이스 레벨 보안**이라 더 안전합니다."

### Q44. Netlify 배포 중 가장 어려웠던 문제는?
> "**의존성 충돌** (zod v4 vs OpenAI)이었습니다. zod v3.23.8로 다운그레이드하고, 빌드 도구를 devDependencies → dependencies로 이동하여 해결했습니다."

### Q45. Zustand를 선택한 이유는?
> "**2KB 크기**로 가볍고, **보일러플레이트가 최소**이며, **TypeScript 완벽 지원**합니다. Redux는 47KB로 과도하고 설정이 복잡합니다."

### Q46. staleTime 5분의 의미는?
> "데이터가 **5분간 fresh 상태**를 유지합니다. 같은 데이터를 5분 내 재조회하면 캐시된 데이터를 즉시 반환하여 불필요한 네트워크 요청을 방지합니다."

### Q47. 번들 크기를 75% 줄인 방법은?
> "**코드 스플리팅**으로 vendor, ai, visualization 등 8개 청크로 분리하고, **esbuild minification**과 **gzip 압축**을 적용하여 1.6MB → 400KB로 감소했습니다."

### Q48. Lighthouse Performance 92점 달성 방법은?
> "1) 코드 스플리팅, 2) 이미지 지연 로딩, 3) React Query 캐싱, 4) TailwindCSS PurgeCSS로 불필요한 CSS 제거. 이를 통해 68점에서 92점으로 향상했습니다."

### Q49. 프로덕션 환경에서 OpenAI API 키를 클라이언트에 노출시키는 게 안전한가요?
> "**현재는 개발용**으로 `dangerouslyAllowBrowser: true`를 사용하지만, 프로덕션에서는 **Edge Functions**로 모든 AI 호출을 이동하여 API 키를 서버 환경에서만 접근하도록 개선할 계획입니다."

### Q50. 이 프로젝트의 확장 가능성은?
> "**B2B SaaS** (팀 협업 기능), **교육 플랫폼** (AI 튜터 시스템), **연구 도구** (논문 관리)로 확장 가능합니다. 서버리스 아키텍처로 **무한 스케일링**이 가능하며, 멀티모달 지원으로 더 다양한 데이터 타입을 처리할 수 있습니다."

---

## 🎯 마무리 팁

### 면접 준비 체크리스트

- [ ] 각 문서의 "면접 포인트" 섹션 암기
- [ ] 구체적인 수치 (temperature 0.7, top-k 3, threshold 0.3) 암기
- [ ] 문제 해결 사례를 STAR 기법으로 준비
- [ ] 프로덕션 URL 접속하여 실제 동작 확인
- [ ] GitHub 코드 리뷰하여 주요 구현 확인

### STAR 기법 예시

**Situation**: PDF.js가 CSP 문제로 작동하지 않음
**Task**: 서버사이드에서 PDF 텍스트를 추출해야 함
**Action**: Supabase Edge Functions + pdf-parse로 재설계
**Result**: 100% 성공률 달성, 평균 처리 시간 2.5초

---

**🎉 축하합니다! 모든 문서를 완성했습니다.**

이 13개 문서는 Synapse AI 지식 관리 시스템의 모든 기술적 측면을 상세히 다루며, 취업 면접에서 자신감 있게 프로젝트를 설명할 수 있도록 준비되었습니다.

**이전 문서**: [12. 프로덕션 배포](./12_프로덕션_배포.md)
**처음으로**: [README](./README.md)
